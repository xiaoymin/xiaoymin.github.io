---
layout: post
title: åŸºäºApple MLXæ¡†æ¶çš„M1è®¾å¤‡ä¸Šå¤§æ¨¡å‹å¾®è°ƒå®è·µ
description: åŸºäºApple MLXæ¡†æ¶çš„M1è®¾å¤‡ä¸Šå¤§æ¨¡å‹å¾®è°ƒå®è·µ
keywords:
- Apple MLXæ¡†æ¶
- M1è®¾å¤‡
- M1è®¾å¤‡å¾®è°ƒ
- å¤§æ¨¡å‹å¾®è°ƒ
- Mistral 7Bæ¨¡å‹
- è®­ç»ƒå‚æ•°è°ƒæ•´
- æ•°æ®é›†å¾®è°ƒ
categories:
- å¤§æ¨¡å‹
sidebar_position: 4
author: å…«ä¸€èœåˆ€
data: 2023å¹´12æœˆ17æ—¥
---

## å‰è¨€

åœ¨ä¸ä¹…å‰è‹¹æœå®˜æ–¹å¼€æºå‘å¸ƒäº†é’ˆå¯¹Apple Silicon èŠ¯ç‰‡ä¼˜åŒ–çš„ MLX æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥ç®€åŒ–ç ”ç©¶äººå‘˜åœ¨ Macã€iPadã€iPhone å¹³å°è®¾è®¡å’Œéƒ¨ç½²æ¨¡å‹çš„è¿‡ç¨‹ã€‚

MLXçš„ä¸»è¦ç‰¹æ€§åŒ…æ‹¬ï¼š

- **ç†Ÿæ‚‰çš„API**ï¼š**MLX** å…·æœ‰ç´§éš NumPy çš„ Python APIã€‚ MLX è¿˜æ‹¥æœ‰åŠŸèƒ½é½å…¨çš„ C++ APIï¼Œå®ƒä¸ Python API éå¸¸ç›¸ä¼¼ã€‚ MLX å…·æœ‰ `mlx.nn` å’Œ `mlx.optimizers` ç­‰æ›´é«˜çº§åˆ«çš„åŒ…ï¼Œå…¶ API ç´§å¯†éµå¾ª PyTorchï¼Œä»¥ç®€åŒ–æ„å»ºæ›´å¤æ‚çš„æ¨¡å‹ã€‚
- **å¯ç»„åˆå‡½æ•°è½¬æ¢**ï¼šMLX å…·æœ‰ç”¨äºè‡ªåŠ¨å¾®åˆ†ã€è‡ªåŠ¨çŸ¢é‡åŒ–å’Œè®¡ç®—å›¾ä¼˜åŒ–çš„å¯ç»„åˆå‡½æ•°è½¬æ¢
- **æƒ°æ€§è®¡ç®— (Lazy computation)**ï¼šMLX ä¸­çš„è®¡ç®—æ˜¯æƒ°æ€§è®¡ç®—ã€‚æ•°ç»„ä»…åœ¨éœ€è¦æ—¶æ‰ä¼šå…·ä½“åŒ–
- **åŠ¨æ€å›¾æ„å»º**ï¼šMLX ä¸­çš„è®¡ç®—å›¾é‡‡ç”¨åŠ¨æ€æ„å»ºï¼Œæ›´æ”¹å‡½æ•°å‚æ•°çš„å½¢çŠ¶ä¸ä¼šè§¦å‘ç¼“æ…¢çš„ç¼–è¯‘ï¼Œå¹¶ä¸”è°ƒè¯•ç®€å•ç›´è§‚
- **å¤šè®¾å¤‡ï¼š**å¯ä»¥åœ¨ä»»ä½•æ”¯æŒçš„è®¾å¤‡ä¸Šè¿è¡Œï¼ˆå½“å‰ä¸º CPU å’Œ GPUï¼‰ï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿå……åˆ†åˆ©ç”¨ç¡¬ä»¶
- **å…·å¤‡ç»Ÿä¸€å†…å­˜ä¼˜åŠ¿**ï¼šMLX å’Œå…¶ä»–æ¡†æ¶çš„æ˜¾ç€åŒºåˆ«æ˜¯é‡‡ç”¨ç»Ÿä¸€å†…å­˜æ¨¡å‹ã€‚ MLX ä¸­çš„æ•°ç»„ä½äºå…±äº«å†…å­˜ä¸­ï¼Œå¯ä»¥åœ¨ä»»ä½•æ”¯æŒçš„è®¾å¤‡ç±»å‹ä¸Šæ‰§è¡Œ MLX é˜µåˆ—ä¸Šçš„æ“ä½œï¼Œè€Œæ— éœ€ç§»åŠ¨æ•°æ®ã€‚

é¡¹ç›®åœ°å€ï¼š[https://github.com/ml-explore/mlx](https://github.com/ml-explore/mlx)

è€Œåœ¨ä»Šå¤©çš„Xä¸Šçœ‹åˆ°Appleå¼€å‘è€…åˆ†äº«è¯´å¯ä»¥åœ¨32GBçš„M1è®¾å¤‡ä¸Šä½¿ç”¨MLXæ¡†æ¶å¯¹Mistral 7B(æˆ–è€…llamA)ç­‰æ¨¡å‹è¿›è¡Œå¾®è°ƒ(Fine-tune)

![image-20231216193342777](/assets/images/llm/apple-mlx-lora-action/image-20231216193342777.png)



## å‡†å¤‡

çœ‹åˆ°å®˜æ–¹çš„ä¾‹å­ï¼Œæˆ‘çš„ç”µè„‘æ­£å¥½æ˜¯M1 32GBçš„é…ç½®ï¼Œå°±æŠŠä»£ç è·‘æ¥è¯•è¯•çœ‹

é¦–å…ˆä»£ç ä¸‹è½½ä¸‹æ¥ï¼Œåœ°å€ï¼š[https://github.com/ml-explore/mlx-examples/tree/main/lora](https://github.com/ml-explore/mlx-examples/tree/main/lora)

å®‰è£…ä¾èµ–ï¼š

```shell
pip install -r requirements.txt
```

ä¸‹è½½Mistral-7B(14.48GBå¤§å°)çš„æ¨¡å‹å¹¶è§£å‹

```shell
curl -O https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar
tar -xf mistral-7B-v0.1.tar
```

å°†ä¸‹è½½ä¸‹æ¥çš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œè½¬æ¢ï¼Œæ‰§è¡Œ`convert.py`æ–‡ä»¶, å‘½ä»¤å¦‚ä¸‹ï¼š

```shell
# è½¬æ¢å‘½ä»¤
python convert.py \
    --torch-model <path_to_torch_model> \
    --mlx-model <path_to_mlx_model>
# è½¬æ¢
python convert.py \
--torch-model mistral-7B-v0.1 \
--mlx-model mistral-7b-v0.1-mlx
```

ä¸¤ä¸ªä¸»è¦çš„å‚æ•°:

- torch-model: Mistralæ¨¡å‹çš„ç›®å½•ï¼Œè§£å‹åä¸ºå½“å‰çš„`mistral-7B-v0.1`
- mlx-model: è¾“å‡ºç›®å½•åç§°ï¼Œè¿™é‡Œå–å`mistral-7b-v0.1-mlx`

é€šè¿‡å‘½ä»¤è½¬æ¢åï¼Œè½¬æ¢çš„ç›®å½•æ–‡ä»¶ä¼šæœ‰ä¸‰ä¸ªæ–‡ä»¶ï¼Œå¦‚ä¸‹å›¾ï¼š

![image-20231216201202978](/assets/images/llm/apple-mlx-lora-action/image-20231216201202978.png)

## å¾®è°ƒ(Fine-tune)

å°†æ¨¡å‹ä¸‹è½½è½¬æ¢å®Œæˆåï¼Œæ¥ä¸‹æ¥å°±å¯ä»¥ä½¿ç”¨å®˜æ–¹æä¾›çš„`lora.py`è¿›è¡Œå¾®è°ƒ(**Fine-tune**)äº†ï¼Œå…ˆæ¥çœ‹æ•°æ®é›†ï¼š

![image-20231216194706972](/assets/images/llm/apple-mlx-lora-action/image-20231216194706972.png)

è®­ç»ƒçš„æ•°æ®é›†æ˜¯1000è¡Œï¼Œä¸»è¦çš„æ ¼å¼ï¼š

> å¾®è°ƒç›®æ ‡æ˜¯å¾—åˆ°ä¸€ä¸ªèƒ½å¤Ÿå°†è‡ªç„¶è¯­è¨€å¥å­è½¬æ¢ä¸ºSQL

```json
{
    "text": "table: 1-1000181-1\ncolumns: State/territory, Text/background colour, Format, Current slogan, Current series, Notes\nQ: Tell me what the notes are for South Australia \nA: SELECT Notes FROM 1-1000181-1 WHERE Current slogan = 'SOUTH AUSTRALIA'"
}
```

æ•°æ®é›†çš„æ ¼å¼å¾ˆæ¸…æ™°ï¼š

```shell
table: è¡¨åç§°
columns: åˆ—åç§°
Q: ç”¨æˆ·é—®é¢˜
A: SQLè¯­å¥
```

## è®­ç»ƒ

åœ¨ç¬¬ä¸€æ¬¡trainçš„è¿‡ç¨‹ä¸­ï¼Œç›´æ¥ä½¿ç”¨demoä¸­çš„å‘½ä»¤ï¼š

```shell
python lora.py --model <path_to_model> \
               --train \
               --iters 600
```

è¿è¡Œäº†å¤§æ¦‚10åˆ†é’Ÿåï¼Œç¨‹åºå°±å¼‚å¸¸é€€å‡ºäº†ï¼Œæç¤ºå†…å­˜ä¸è¶³ã€‚

![image-20231216195734079](/assets/images/llm/apple-mlx-lora-action/image-20231216195734079.png)

ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºï¼Œåœ¨å£°æ˜å†…å­˜çš„è¿‡ç¨‹ä¸­ï¼Œå‡ºç°äº†å¼‚å¸¸ï¼Œæ— æ³•å¼€è¾Ÿæ–°å†…å­˜ç©ºé—´ï¼Œå¹¶ä¸”æ¯ç§’çš„Tokensæ•°é‡ä¹Ÿå¾ˆæ„ŸäººğŸ˜­

åœ¨çœ‹äº†å®˜æ–¹çš„é’ˆå¯¹å†…å­˜çš„issueså»ºè®®åï¼Œå‘ç°æœ‰å‡ ä¸ªå‚æ•°æ˜¯å½±å“ç€å†…å­˜ä½¿ç”¨çš„ï¼š

- **--batch-size**ï¼šå°è¯•é€šè¿‡ `--batch-size` ä½¿ç”¨è¾ƒå°çš„æ‰¹é‡å¤§å°ã€‚ é»˜è®¤å€¼ä¸º 4ï¼Œå› æ­¤å°†å…¶è®¾ç½®ä¸º 2 æˆ– 1 å°†å‡å°‘å†…å­˜æ¶ˆè€—ã€‚ è¿™å¯èƒ½ä¼šå‡æ…¢é€Ÿåº¦ï¼Œä½†ä¹Ÿä¼šå‡å°‘å†…å­˜ä½¿ç”¨ã€‚
- **--lora-layers**:å°‘å±‚æ•°ä»¥ä½¿ç”¨ `--lora-layers` è¿›è¡Œå¾®è°ƒã€‚ é»˜è®¤å€¼ä¸º 16ï¼Œå› æ­¤æ‚¨å¯ä»¥å°è¯• 8 æˆ– 4ã€‚è¿™ä¼šå‡å°‘åå‘ä¼ æ’­æ‰€éœ€çš„å†…å­˜é‡ã€‚ å¦‚æœæ‚¨ä½¿ç”¨å¤§é‡æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå®ƒè¿˜å¯èƒ½ä¼šé™ä½å¾®è°ƒæ¨¡å‹çš„è´¨é‡ã€‚
- æ•°æ®é›†ï¼šè¾ƒé•¿çš„ç¤ºä¾‹éœ€è¦æ›´å¤šçš„å†…å­˜ã€‚ å¦‚æœè¿™å¯¹æ‚¨çš„æ•°æ®æœ‰æ„ä¹‰ï¼Œæ‚¨å¯ä»¥åšçš„ä¸€ä»¶äº‹æ˜¯åœ¨åˆ¶ä½œ {train, valid, test}.jsonl æ–‡ä»¶æ—¶å°†ç¤ºä¾‹åˆ†è§£ä¸ºæ›´å°çš„åºåˆ—ã€‚

æ ¹æ®å®˜æ–¹çš„å»ºè®®ï¼Œé‚£ä¹ˆä¿®æ”¹trainå‚æ•°ï¼Œå¦‚ä¸‹ï¼š

```shell
python lora.py \
   --model mistral-7b-v0.1-mlx \
   --train \
   --batch-size 1 \
   --lora-layers 4
```

æŒ‰è¿™ä¸ªå‘½ä»¤æ‰§è¡Œåï¼Œåœ¨æˆ‘çš„M1è®¾å¤‡ä¸Šæ‰§è¡Œçš„è¿˜æ¯”è¾ƒå¿«ï¼Œæ¯ç§’çš„Tokensæ•°é‡å¹³å‡ä¸Š110å·¦å³

![image-20231216200532260](/assets/images/llm/apple-mlx-lora-action/image-20231216200532260.png)

è€ŒLossçš„å€¼å¦‚ä¸‹ï¼š

| Iter | Loss  |
| ---- | ----- |
| 1    | 2.265 |
| 200  | 1.516 |
| 400  | 1.380 |
| 600  | 1.350 |
| 800  | 1.325 |

trainå®Œæˆåï¼Œä¼šåœ¨æœ¬åœ°é»˜è®¤ç”Ÿæˆä¸€ä¸ªæƒé‡æ–‡ä»¶`adapters.npz`

æµ‹è¯•ç»“æœï¼š

```shell
python lora.py --model mistral-7b-v0.1-mlx \
               --adapter-file adapters.npz \
               --num-tokens 50 \
               --prompt "table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: "
Loading pretrained model
Total parameters 7243.436M
Trainable parameters 1.704M
Loading datasets
Generating
table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
# å¤§æ¨¡å‹è¾“å‡º
A:  SELECT Nationality FROM 1-10015132-16 WHERE Player = 'Terrence Ross' blowing off the rosshill. SELECT Nationality FROM 1-10015
```

ä»ç»“æœçœ‹ï¼ŒSQLçš„å‰åŠéƒ¨åˆ†å†™å¯¹äº†ï¼Œå¹¶ä¸”ä¹Ÿè¯†åˆ«å‡ºäº†å­—æ®µã€whereæ¡ä»¶ï¼Œä½†æ˜¯åé¢çš„å¥å­å¥½åƒå°±ä¸å¤ªå¯¹äº†

æˆ‘æ€€ç–‘æ˜¯åœ¨trainæ—¶ï¼Œå‚æ•°`--lora-layers 4`çš„é—®é¢˜ï¼Œè¿™æ—¶ï¼Œæˆ‘å°†æ”¹å‚æ•°æ”¹ä¸º8ï¼Œåœ¨trainä¸€æ¬¡

```shell
python lora.py \
   --model mistral-7b-v0.1-mlx \
   --train \
   --adapter-file adapters_2_8_1.npz \
   --batch-size 2 \
   --lora-layers 8
```

è€ŒLossçš„å€¼å¦‚ä¸‹ï¼š

| Iter | loss  |
| ---- | ----- |
| 1    | 2.348 |
| 200  | 1.392 |
| 400  | 1.293 |
| 800  | 1.213 |
| 1000 | 1.233 |

ä¹‹åï¼ŒåŒæ ·çš„å‘½ä»¤ï¼Œå†æ¥çœ‹æ•ˆæœï¼š

```shell
python lora.py --model mistral-7b-v0.1-mlx \
               --adapter-file adapters_2_8.npz \
               --num-tokens 50 \
               --prompt "table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A: "
Loading pretrained model
Total parameters 7243.436M
Trainable parameters 1.704M
Loading datasets
Generating
table: 1-10015132-16
columns: Player, No., Nationality, Position, Years in Toronto, School/Club Team
Q: What is terrence ross' nationality
A:  SELECT Nationality FROM 1-10015132-16 WHERE Player = 'Terrence Ross' SELECT Nationality FROM 1-10015132-16 WHERE
```

çœ‹æ•ˆæœå¥½åƒåœ¨SQLè¯­å¥ä¸­ï¼Œæ¯”ä¸Šé¢çš„æ•ˆæœç¨å¾®è¦å¥½ä¸€ç‚¹äº†?ä½†æ˜¯ç»“æœè¿˜æ˜¯ä¸å¯¹ã€‚

æ•ˆæœå¹¶æ²¡æœ‰è¾¾åˆ°é¢„æœŸï¼Œæˆ‘è§‰å¾—ä¸»è¦æ˜¯å¯èƒ½æœ‰å‡ ä¸ªæ–¹é¢çš„åŸå› ï¼š

- è®­ç»ƒæ•°æ®é›†å¤ªå°‘ï¼Œå¯¼è‡´å¤§æ¨¡å‹å¯èƒ½æ— æ³•,train.jsonlä¸­çš„æ•°æ®é›†æ˜¯1000
- å‚æ•°`--lora-layers `çš„é—®é¢˜ï¼Œé»˜è®¤æ˜¯16ï¼Œè™½ç„¶æˆ‘æœ€åæ”¹æˆäº†8ï¼Œä½†æ˜¯ä»å®˜æ–¹ç»™å‡ºçš„è¯´æ˜æ¥çœ‹ï¼Œè¯¥å‚æ•°ä¼šå½±å“è´¨é‡

æˆ‘å°†å‚æ•°`--lora-layers `ä¿®æ”¹ä¸º16è¿›è¡Œäº†å°è¯•ï¼Œè·‘ä¸äº†ï¼Œå¯èƒ½è¿˜æ˜¯æˆ‘çš„å†…å­˜å¤ªä½äº†ğŸ˜­ï¼Œé‚£æˆ‘åªèƒ½åŠ æ•°æ®é›†äº†

ä¿®æ”¹äº†dataç›®å½•ä¸‹çš„wikisql.pyæ–‡ä»¶ï¼Œå°†æ•°æ®é›†ä¸‹è½½æ•´ç†çš„æ€»ä½“æ•°é‡ä¸Šå‡åˆ°10000ï¼Œä»£ç ï¼š

```python

if __name__ == "__main__":
    datanames = ["train", "dev", "test"]
    sizes = [56355, 8421, 15878]
    for dataname, size in zip(datanames, sizes):
        len(WikiSQL(dataname)) == size, f"Wrong {dataname} set size."

    # Write the sets to jsonl
    import json

    train, dev, test = load()
    # æ­¤å¤„åŸtrainå‚æ•°æ˜¯1000ï¼Œæˆ‘æ”¹æˆ5000
    datasets = [
        (train, "train", 10000),
        (dev, "valid", 1000),
        (test, "test", 1000),
    ]
    for dataset, name, size in datasets:
        with open(f"data/{name}.jsonl", "w") as fid:
            for e, t in zip(range(size), dataset):
                # Strip the <s>, </s> since the tokenizer adds them
                json.dump({"text": t[3:-4]}, fid)
                fid.write("\n")
```

ä¿®æ”¹æ•°æ®é›†åï¼Œåœ¨trainè¿‡åï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„æƒé‡æ–‡ä»¶ï¼Œå‘½ä»¤ï¼š

```shell
python lora.py \
   --model mistral-7b-v0.1-mlx \
   --train \
   --adapter-file adapters_2_8_1.npz \
   --batch-size 2 \
   --lora-layers 8
```

lossçš„trainè¿‡ç¨‹åˆ†å€¼å˜åŒ–ï¼š

| Iter | loss  |
| ---- | ----- |
| 1    | 2.348 |
| 200  | 1.472 |
| 400  | 1.410 |
| 600  | 1.387 |
| 800  | 1.360 |
| 1000 | 1.349 |

å†æ¥çœ‹çœ‹æˆ‘ä»¬çš„promtå¾—åˆ°çš„ç»“æœï¼š

![image-20231217105402278](/assets/images/llm/apple-mlx-lora-action/image-20231217105402278.png)

ä»ç»“æœæ¥çœ‹ï¼ŒSQLè¯­å¥çš„è¯­æ³•å¥½åƒå¹¶æ²¡æœ‰ä»€ä¹ˆå¤§çš„é—®é¢˜ï¼Œåªæ˜¯ç»“æœæ²¡æœ‰è¾¾åˆ°é¢„æœŸï¼Œå¯èƒ½è¿˜æ˜¯å¾—ä»æ•°æ®é›†åŠç›¸å…³å‚æ•°æ‰¾ä¸€ä¸‹åŸå› ã€‚

## ç»“è®º

è™½ç„¶è¿è¡Œçš„ç»“æœè¿˜æ²¡æœ‰å®Œå…¨è¾¾åˆ°é¢„æœŸï¼Œä½†æ˜¯åœ¨MACä¸Šé€šè¿‡Appleæ¨å‡ºçš„MLXæ·±åº¦å­¦ä¹ æ¡†æ¶è¿›è¡ŒFine-tureçš„æŠ€æœ¯æ–¹æ¡ˆæ˜¯å¯è¡Œçš„ã€‚

è¿™ä¹Ÿä¸ºä»¥åå¤§æ¨¡å‹çš„è®­ç»ƒã€ç”Ÿæ€å‘å±•æä¾›äº†å¦å¤–ä¸€ç§å¯èƒ½æ€§ã€‚

åŒ…æ‹¬æˆ‘ä»¬åº”ç”¨å¼€å‘è€…åœ¨åšRAGçš„è¿‡ç¨‹ä¸­ï¼Œå’Œæ•°æ®è¿›è¡Œå¯¹è¯çš„åœºæ™¯éšç€ä¸šåŠ¡çš„æ·±å…¥è‚¯å®šä¼šè§¦åŠï¼Œè€Œå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒæ˜¯ä¸å¯é¿å…çš„ã€‚

## Reference

- [https://github.com/ml-explore/mlx-examples/tree/main/lora](https://github.com/ml-explore/mlx-examples/tree/main/lora)
- [https://github.com/ml-explore/mlx](https://github.com/ml-explore/mlx)
- [https://twitter.com/awnihannun/status/1735782998623261071](https://twitter.com/awnihannun/status/1735782998623261071)